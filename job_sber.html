<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />
    <title>Description of the Sber projects</title>
    <meta name="description" content="description of the Sber projects" />

    <link rel="stylesheet" href="css/style.css" />

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@400;600;700;900&display=swap"
      rel="stylesheet"
    />
  </head>
  <body>
    <header class="header">
      <div class="header__content">
        <div class="header__logo-container">
          <div class="header__logo-img-cont">
            <img
              src="./assets/png/profile.png"
              alt="Ram Maheshwari Logo Image"
              class="header__logo-img"
            />
          </div>
          <span class="header__logo-sub">Polina Tanasevich</span>
        </div>
        <div class="header__main">
          <ul class="header__links">
            <li class="header__link-wrapper">
              <a href="./index.html" class="header__link"> Home </a>
            </li>
            <li class="header__link-wrapper">
              <a href="./index.html#about" class="header__link">About </a>
            </li>
            <li class="header__link-wrapper">
              <a href="./index.html#conferences" class="header__link">
                Conferences
              </a>
            </li>
            <li class="header__link-wrapper">
              <a href="./index.html#contact" class="header__link"> Contact </a>
            </li>
          </ul>
          <div class="header__main-ham-menu-cont">
            <img
              src="./assets/svg/ham-menu.svg"
              alt="hamburger menu"
              class="header__main-ham-menu"
            />
            <img
              src="./assets/svg/ham-menu-close.svg"
              alt="hamburger menu close"
              class="header__main-ham-menu-close d-none"
            />
          </div>
        </div>
      </div>
      <div class="header__sm-menu">
        <div class="header__sm-menu-content">
          <ul class="header__sm-menu-links">
            <li class="header__sm-menu-link">
              <a href="./index.html"> Home </a>
            </li>

            <li class="header__sm-menu-link">
              <a href="./index.html#about"> About </a>
            </li>

            <li class="header__sm-menu-link">
              <a href="./index.html#conferences"> Conferences </a>
            </li>

            <li class="header__sm-menu-link">
              <a href="./index.html#contact"> Contact </a>
            </li>
          </ul>
        </div>
      </div>
    </header>
    <section class="project-cs-hero">
      <div class="project-cs-hero__content">
        <h1 class="heading-primary">Tech Lead DS</h1>
        <div class="project-cs-hero__info">
          <p class="text-primary">
            Leading cross-functional teams in building AI solutions like geospatial analytics models, 
            driving scalable automation, aligning AI with business goals, and contributing to industry research.
          </p>
        </div>
        <div class="project-cs-hero__cta">
          <a href="https://github.com/TanasevichPS/geo_embeddings" class="btn btn--bg" target="_blank">Private Repo</a>
        </div>
      </div>
    </section>
    <section class="project-details">
      <div class="main-container">
        <div class="project-details__content">
          <div class="project-details__showcase-img-cont">
            <img
              src="./assets/jpeg/nte_start.png"
              alt="Project Image"
              class="project-details__showcase-img"
            />
          </div>
          <div class="project-details__content-main">
            <div class="project-details__desc">
              <h3 class="project-details__content-title">Overview</h3>
              <p class="project-details__desc-para">
                <strong> 1.	Developed geo-analytics system, which was then improved with a custom LLM solution. 
                This boosted the accuracy of the geo-analytics forecasts by 16%. </strong> <br/>
                 <strong> 1.	Developed geo-analytics system, which was then improved with a custom LLM solution. 
                This boosted the accuracy of the geo-analytics forecasts by 16%. </strong> <br/>
                Постановка задачи и бизнес-проблема<br/>
                Клиент стоял перед необходимостью повысить точность прогнозирования регионального спроса, объединяя разрозненные данные:
                гео­транзакции, точки интереса, тип локации, временные метки и пр., а также отзывы пользователей. 
                Ключевая проблема заключалась в том, что классические статистические методы «поъедали» большую часть нюансов мультимодальных данных: 
                пространственное распределение покупателей, особенности разных типов локаций (ТЦ, офисные зоны, жилые кварталы) и текстовые отзывы. 
                Без учёта всех этих факторов модель недооценивала локальные всплески спроса, что приводило к завышенному или заниженному запасу товаров и, 
                как следствие, потерям в продажах и логистике. <br/>
                </p>
              <p class="project-details__desc-para">
                Подход к решению <br/>
                - Мультимодальные эмбеддинги событийных данных <br/>
                Использовала библиотеку PyTorch Lifestream для построения последовательностей событий.<br/>
                Применила три метода контрастивного обучения: COLES, NSP (Next Sentence Prediction) и SOP (Sentence Order Prediction) — 
                чтобы модели «поняли» связи между транзакциями и визитами в точки интереса. <br/>
                Полученные эмбеддинги сжали до 128 D с помощью PCA (95 % сохранённой дисперсии) — компромисс между скоростью 
                обучения инференса и сохранением информации.<br/>
                Почему именно PCA? Альтернативы (т. н. автоэнкодеры) оказались слишком «тяжёлыми» для дальнейшего объединения с текстовыми эмбеддингами.<br/>
                </p>
              <p class="project-details__desc-para">
                Интеграция отзывов через LLM <br/>
                Добавила текстовые отзывы из яндекс карт. Чтобы не тратить ресурсы на дообучение, выбрала инференс-режим без fine-tuning.<br/>
                Использовала три модели для сравнения:<br/>
                - GigaChat (проприетарная, «как есть»),
                - LLaMA‑2 (7B) в INT8-квантовании через BitsAndBytes (сжатие 4×),<br/>
                - Mistral 7B также в INT8 (сжатие 4×).<br/>
                Эмбеддинги получали через mean pooling последнего скрытого слоя.<br/>
                Почему INT8? Четырёхкратное снижение размера модели ускорило инференс и упростило развёртывание на узлах с ограниченными ресурсами.<br/>
                 Bitsandbytes - Библиотека для квантования моделей (сжатие весов 32/16-битных чисел в 8/4-битные). 
                Уменьшает размер модели и ускоряет инференс<br/>
                </p>
              <p class="project-details__desc-para">
                Гибридная модель<br/>
                Сконкатенировала 128‑мерные мультимодальные эмбеддинги и 1024‑мерные LLM‑эмбеддинги.<br/>
                На выходе получили вектор размерности 1152 D, подающийся на downstream‑модели (регрессор для прогнозирования спроса, а также классификатор отзывов).<br/>
                </p>
              <p class="project-details__desc-para">
                Детали реализации и процесс коммуникации<br/>
                Команда и роли <br/>
                - Data Engineering: сбор и предобработка гео­данных и транзакций.<br/>
                - (Я) ML‑инженер: разработка пайплайна контрастивного обучения и LLM‑эмбеддингов.<br/>
                - DevOps: развёртывание PCA‑моделей и INT8‑LLM в Kubernetes‑кластере. <br/>
                - (Я) Продакт‑менеджмент: сбор требований, постановка MVP, приемка результата.<br/>
                - Заказчик (маркетинг + логистика): регулярные демо‑сессии, уточнение критериев оценки (MAPE, R²).<br/>
                <br/>
                
                Процесс<br/>

                - Сбор требований: две недели интервью с маркетологами и аналитиками региональных продаж.<br/>                
                - Прототип: в течение месяца построили простой пайплайн для событийных эмбеддингов (без текстов). Провели A/B‑тесты на исторических данных — 
                получили снижение MAPE на 8 %.<br/>                
                - Расширение на отзывы: параллельно с DevOps запустили эксперименты с LLaMA‑2 и Mistral в INT8. 
                Итеративно меняли параметры квантования, фиксировали качество downstream‑задач.<br/>                
                - Интеграция PCA и LLM: собрали гибридный вектор, обучили финальный регрессор. <br/>                
                - Тестирование на «живых» данных: в течение месяца модель работала в shadow‑режиме рядом с текущей системой.<br/>                
                - Внедрение в продакшен: автоматизированный деплой в контейнерах, настройка мониторинга качества прогноза.<br/>
                </p>
              <p class="project-details__desc-para">
                Ограничения и сложности<br/>               
                Ограниченный бюджет GPU: INT8‑квантование было обязательным, иначе инференс LLM отнимал бы 70 % всего GPU‑времени.<br/>                
                Разрозненные источники данных: гео‑данные поставлялись в трёх форматах (JSON, Parquet, CSV), требовалось выстроить единый CDC‑пайплайн.<br/>                   
                Коммуникация: маркетологи просили «быстрее и побольше фич», а DevOps требовал стабильности и простоты. Закрыли конфликт регулярными митингами дважды 
                в неделю и чёткими тикетами в Jira.<br/>
                </p>
              <p class="project-details__desc-para">
                Ключевые результаты<br/>                
                - Снижение MAPE на 16 % по сравнению с baseline только на мультимодальных эмбеддингах.<br/>                
                - Повышение R² прогноза спроса с 0.81 до 0.91 (+12 %) благодаря интеграции LLM‑эмбеддингов (наиболее эффективна квантованная LLaMA‑2).<br/>                
                - Ускорение инференса LLM в 4× за счёт INT8‑квантования, что позволило обслуживать региональные прогнозы в реальном времени.<br/>          
                </p>
              <p class="project-details__desc-para">
                Что было самым сложным и как решали проблемы<br/>                
                Интеграция разнородных данных: выстроили modular ETL‑пайплайн с проверками качества на каждом этапе.<br/>                
                Стабильность LLM‑фреймворков: перешли на Docker‑образы с pinned‑версиями библиотек и ввели автоматические smoke‑тесты при каждом релизе.<br/>                
                Баланс качества и скорости: провели grid search по числу компонент PCA и битности квантования, чтобы найти «золотую середину». <br/>
                </p>
              <p class="project-details__desc-para">
                Поэтапный ход работ<br/>
                
                Запрос: “Нужна система прогноза спроса с учётом геоданных и отзывов.”<br/>                
                Анализ данных: аудит форматов, оценка объёма, spikes в транзакциях.<br/>                
                MVP мультимодальности: контрастивные эмбеддинги → первое снижение MAPE.<br/>                
                Эксперименты с LLM: выбор моделей → оценка downstream‑качества.<br/>                
                Сборка гибридной модели: конкатенация эмбеддингов → финальный регрессор.<br/>                
                Shadow‑режим: параллельный запуск без влияния на бизнес.<br/>                
                Внедрение: автоматический деплой, мониторинг, отчётность перед стейкхолдерами.<br/>                
                Итерации и улучшения: каждые две недели добавлялись новые типы событий и обновлялись LLM‑версии.<br/>
                </p>
              <p class="project-details__desc-para">
                Таким образом, проект прошёл путь от «разрозненных таблиц» до высокоточного гибридного решения, обеспечив значительный прирост 
                качества прогнозов и ускорение рабочих процессов.
                </p>
              <p class="project-details__desc-para">                             
                <p>Пример реализации:</p>
                <p><code>
                  from transformers import AutoModelForCausalLM, AutoTokenizer
                  import torch
                  # Загрузка модели и токенизатора<Br>
  
                  model_id = "meta-llama/Llama-2-7b-chat-hf"<Br>
                  tokenizer = AutoTokenizer.from_pretrained(model_id)<Br>
                  model = AutoModelForCausalLM.from_pretrained(<Br>
                    model_id,<Br>
                    load_in_8bit=True,  # Включение 8-битного квантования <Br>
                    device_map="auto"    # Автораспределение по GPU/CPU) <Br>
                  # Пример инференса <Br>
                  input_text = "Как повысить точность геоаналитики?" <Br>
                  inputs = tokenizer(input_text, return_tensors="pt").to("cuda") <Br>
                  outputs = model.generate(inputs, max_new_tokens=50)<Br>
                  print(tokenizer.decode(outputs[0], skip_special_tokens=True)) <Br>
  
                  # Для эмбеддингов: <Br>
                  inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True).to("cuda") <Br>
                  with torch.no_grad(): <Br>
                    outputs = model(inputs, output_hidden_states=True) <Br>
  
                  # Усреднение последнего слоя<Br>
                  embeddings=outputs.hidden_states[-1].mean(dim=1).cpu().numpy()
                </code></p>
              </p>
              <p class="project-details__desc-para">


                
            <strong> 2.	Accelerated BERT inference 2x on NVIDIA A100 via model quantization. </strong> <br/>
                Проблема и постановка задачи<br/>
            Бизнес‑контекст: создавался высоконагруженный чат‑бот для обработки тысяч запросов пользователей в реальном времени. 
                Пользователи требовали мгновенных ответов, а SLА по задержке был не более 30 мс. <br/>
            
            Основная проблема: стандартный FP32‑BERT при batch size=16 показывал латентность ~50 мс на NVIDIA A100, что в пиковые часы приводило к очередям 
                запросов и ухудшению пользовательского опыта. Пропускная способность ограничивала работу сервиса до ~20 RPS (requests per second).<br/>
            
            Цель проекта: сократить задержку вдвое (≤25 мс) и увеличить throughput до ≥40 RPS, сохранив качество ответов в пределах допустимого снижения метрик.<br/>
            </p>
          <p class="project-details__desc-para">
            Подход к решению и обоснование выбора<br/>
            Квантование модели до INT8<br/>            
            Почему INT8?<br/>            
            4× уменьшение объёма весов и активаций ≈ в 4 раза меньше вычислений и меньшая нагрузка на память.<br/>            
            Хорошо поддерживается на NVIDIA A100 через тензорные ядра с оптимизированными INT8‑примитивами.<br/>
            </p>
          <p class="project-details__desc-para">
            Метод калибровки<br/>            
            - Собрали репрезентативный калибровочный датасет из реальных запросов чат‑бота (~10 000 примеров).<br/>            
            - Использовали post‑training static quantization: прогнали датасет через FP32‑модель, 
                собрали статистику активаций и рассчитали оптимальные масштабные коэффициенты для каждого тензора.<br/>            
            - Минимизировали потерю точности при квантовании активаций (выбор порогов 99.9 % перцентили).<br/>
                </p>
          <p class="project-details__desc-para">
            Оптимизация инференса через TensorRT<br/>            
            Конвертация графа<br/>            
            Перевели BERT из PyTorch в ONNX, затем развернули в TensorRT Engine с включённой поддержкой INT8.<br/>       
                
            Динамический батчинг<br/>            
            Настроили интеграцию с сервером инференса (TensorRT Inference Server), чтобы автоматически агрегировать мелкие запросы в 
                батчи до 16 штук, без ручного ожидания.<br/>            
            Использование тензорных ядер A100<br/>            
            TensorRT Torch‑ и cuBLAS‑плагины обеспечили векторизованные и матричные операции в INT8, что дало к значительному ускорению мат mul и attention.<br/>
            
            Детали реализации и коммуникация<br/>
            Команда<br/>            
            - (Я) ML‑инженер: подготовка ONNX‑модели, настройка калибровки и пайплайна TensorRT.<br/>            
            - DevOps‑инженеры: развёртывание TensorRT Inference Server в Kubernetes‑кластере, настройка GPU‑ресурсов (A100), мониторинг метрик.<br/>            
            - QA‑инженеры: проверка стабильности и регрессий качества (ROUGE‑L, Perplexity) на валидационном наборе.<br/>            
            - (Я) Продакт‑менеджер: сбор требований, контроль сроков, презентации заказчику.<br/>
            </p>
          <p class="project-details__desc-para">
            Процесс и этапы<br/>            
            Сбор данных для калибровки (1 неделя)<br/>            
            Эксперименты с калибровкой INT8 (2 недели): анализ потерь точности при разных перцентилях, выбор оптимальных настроек.<br/>            
            ONNX → TensorRT Engine (1 неделя): проверка стабильности, устранение несовместимостей слоёв.<br/>            
            Интеграция динамического батчинга (1 неделя): настройка сервера, нагрузочное тестирование.<br/>            
            Регрессионное тестирование качества (1 неделя): оценка ROUGE-L и Perplexity на наборе из 5 000 диалогов.<br/>            
            Shadow‑режим в продакшене (2 недели): сравнение с FP32‑результатом без влияния на пользователей.<br/>            
            Окончательное внедрение и мониторинг (1 неделя): автоматическое переключение трафика, настройка алертов при деградации качества.<br/>
            </p>
          <p class="project-details__desc-para">
            Ограничения и возникшие сложности<br/>
            Сложности при калибровке:<br/>            
            В начале модель при INT8 показывала значительное падение Perplexity (с 16 до 28). 
                Решение: увеличили размер калибровочного набора и перцентиль порога до 99.9 %, что вернуло Perplexity к 19.<br/>            
            Проблемы совместимости ONNX → TensorRT: <br/>                        
            Баланс качества/скорости:<br/>            
            Итоговое снижение ROUGE‑L F1 с 0.62 до 0.58 было приемлемо для бизнес‑задачи, но команда заказчика настаивала на пороге не ниже 0.59. 
            Достигли компромисса, вернув часть измерений в FP16, а не INT8, на ключевых слоях (mixed precision), 
                что позволило поднять ROUGE‑L до 0.59 при латентности ~28 мс.<br/>
            </p>
          <p class="project-details__desc-para">
            Ключевые результаты<br/>
            <table>
            <tr>
                <td>Метрика</td>
                <td>До оптимизации</td>
                <td>После оптимизации</td>
            </tr>
            <tr>
                <td>Latency </td>
                <td>50 mc </td>
                <td>25 mc</td>
            </tr>
            <tr>
                <td>Throughput</td>
                <td>20 RPS</td>
                <td>40 RPS</td>
            </tr>
            <tr>
                <td>ROUGE-L (F1)</td>
                <td>0.62</td>
                <td>0.58</td>
            </tr>
            <tr>
              <td>Perplexity</td>
              <td>16</td>
              <td>19</td>
          </tr>
        </table> 
          </p>
      <p class="project-details__desc-para"> 
        ROUGE-L: Оценивает длину наибольшей общей подпоследовательности (LCS) слов, учитывая беглость и структуру. <br/> 
        Perplexity – метрика, измеряющая неуверенность языковой модели (LLM) при генерации текста. экспонента от энтропии: 
        Perplexity=exp(−1/N∑logP(w_i∣w1,…,wi−1)), где: P(wi) — вероятность предсказания слова w_i моделью. 
        N — количество слов в тексте. <br/> 
        Метрики ROUGE-L и Perplexity оценивались на отдельном валидационном наборе диалогов, репрезентативном для работы чат-бота. 
            </p>
          <p class="project-details__desc-para">
            Наиболее трудные моменты и их решение<br/>
            Управление ожиданиями стейкхолдеров: маркетологи ожидали сохранения качества на уровне FP32; 
            организовали демонстрацию A/B‑тестов и объяснили преимущества latency‑критичных задач, добившись согласия на небольшой компромисс по ROUGE‑L.<br/>            
            Мониторинг в продакшене: настроили метрики Latency и Perplexity с алертами при отклонении более чем на 10 % от baseline, 
                что позволило вовремя обнаруживать деградации после обновлений.<br/>
            </p>
          <p class="project-details__desc-para">
                
            Таким образом, комбинированный подход: INT8‑квантование при помощи TensorRT + динамический батчинг + частичный mixed precision
            позволил ускорить инференс BERT вдвое, удвоить пропускную способность и при этом сохранить качество ответов на приемлемом уровне
            </p>
    <p class="project-details__desc-para">
        
      <strong> 3.	Built NLP pipeline for log analysis that detected +42% errors, resolved 70% of issues, cut verification costs by 80%, and
      improved customer experience. </strong> <br/>
            Проблема и цель<br/>
            В компании использовалась монолитная система логирования, где текстовые логи операционных сервисов хранились в сыром виде. 
            Инженеры вручную фильтровали и анализировали эти записи, чтобы находить и исправлять ошибки: <br/>            
            - Низкая видимость. Обычные алерты ловили лишь ~40 % реальных ошибок. Оставшиеся “тихие” сбои доставались критическим инцидентам.<br/>            
            - Высокие затраты труда. На разбор и верификацию логов уходило до 5 инженеро‑часов в день на одну команду.<br/>            
            - Отложенное реагирование. Ошибки выявлялись с запозданием до нескольких дней, что влияло на SLA и ухудшало пользовательский опыт. <br/>
            </p>
    <p class="project-details__desc-para">
            Цель проекта — создать сквозной пайплайн, который автоматически:<br/>            
            - Предобрабатывает логи (очистка, токенизация, PII‑маскирование). <br/>            
            - Обнаруживает и классифицирует ошибки на лету. <br/>            
            - Оценивает их влияние на бизнес‑процессы (время простоя, дополнительные действия сотрудников).<br/>            
            - Выстраивает граф клиентских путей, чтобы приоритизировать исправления.<br/>
            </p>
    <p class="project-details__desc-para">
            Подход к решению и обоснование<br/>
            Автоматизация вместо ручной верификации. Ручная обработка не масштабируется: рост нагрузки ведёт к пропущенным инцидентам.<br/>            
            NLP‑методы для гибкости. Правила (регулярки) не покрывали нетипичные сообщения, поэтому я добавила NER‑модель Natasha для распознавания 
            сервисных сущностей и терминов.<br/>            
            Графовая аналитика для приоритезации. Простое счётное ранжирование ошибок не учитывает, как они “лепятся” по цепочке действий пользователя. 
            Построила граф клиентских сессий, чтобы выявлять узкие места.<br/>
            </p>
    <p class="project-details__desc-para">
            Детали реализации<br/>
            Сбор и хранение логов<br/>            
            Логи поступают из Kafka‑топиков, парсятся на уровне строчки в лёгкую JSON‑структуру.<br/>            
            Ингестируются в ElasticSearch для хранения “сырых” данных.<br/>            
            Предобработка текста <br/>            
            - Очистка и нормализация: Python‑скрипты с регулярными выражениями удаляют шум (ISO‑таймстемпы, UUID).<br/>
            - Tokenization: NLTK разбивает строки на токены, заботясь о служебных символах и тд. <br/>
            </p>
    <p class="project-details__desc-para">
            PII‑маскирование:<br/>            
            Natasha находит имена, пользовательские идентификаторы, email‑адреса и заменяет их масками.<br/>            
            Дополняем NER‑результаты регулярными выражениями для числовых ID и IP‑адресов.<br/>            
            Обнаружение и классификация ошибок<br/>            
            Keyword‑based filtering: список часто встречающихся “красных” паттернов (Exception, Stacktrace).<br/>            
            Topic modeling (Gensim LDA): выделяем аномальные темы, которые неожиданно “вспыхнули” в логах, и проверяем их на ошибки.<br/>            
            ML‑классификатор: после LDA в качестве признаков подаём токены и NER‑метки в LightGBM, обученный на исторически размеченных логах.<br/>
            </p>
    <p class="project-details__desc-para">
            Оценка влияния на бизнес‑процессы<br/>            
            Считаем задержку: разница между временными метками первого и последнего лога, относящегося к одной сессии/ошибке.<br/>            
            Метрика “времени простоя инженера”: суммируем перерывы между шагами бизнес‑процесса, вызванные ошибками (из логов workflow).<br/>            
            Визуализация и граф клиентских путей<br/>            
            Собираем путь пользователя как ориентированный граф с узлами «событие→событие».<br/>            
            NetworkX генерирует граф, а Gephi строит дашборд для анализа “узких мест” — точек скопления ошибок, влияющих на большинство путей.<br/>
            </p>
    <p class="project-details__desc-para">
            Процесс, коммуникация и ограничения<br/>
            Команда и роли<br/>            
            - Data Engineers — сбор, очистка и подача логов из Kafka в ES.<br/>
            - (Я) NLP‑инженер — предобработка текста, NER‑интеграция, построение LDA и классификатора.<br/>            
            - (Я) BI‑аналитики — визуализация графов и построение дашбордов в Gephi.<br/>            
            - DevOps — развёртывание пайплайна в Kubernetes, мониторинг и алерты.<br/>            
            - Сервисная команда (SME) — проверка корректности распознанных ошибок и апелляция к бизнес‑контексту.
            </p>
    <p class="project-details__desc-para">
            Этапы<br/>            
            Сбор требований (1 неделя): интервью с командой поддержки и аналитиками SLA.<br/>            
            Прототип предобработки (2 недели): реализовали регулярки и базовый NER.<br/>            
            Эксперименты с LDA и LightGBM (3 недели): добились +42 % детектируемых ошибок.<br/>            
            Построение графа клиентских путей (2 недели): согласовали метрики влияния.<br/>            
            Интеграция и тестирование в shadow‑режиме (2 недели): сравнивали автоматически найденные ошибки с ручными инцидентами.<br/>            
            Внедрение и обучение персонала (1 неделя): провели тренинг службы поддержки и выдали мануалы.<br/>
            </p>
    <p class="project-details__desc-para">
            Ограничения<br/>                   
            Скорость обработки. При пике 10 000 msg/sec пришлось масштабировать Kafka‑консьюмеры в 4 реплики.<br/>            
            Персональные данные. Требовалась строгая GDPR‑комплайенс: NER + регулярки обязаны замаскировать PII без ошибок.<br/>
            </p>
    <p class="project-details__desc-para">
            Трудности и их решение<br/>
            - Неполнота NER‑модели “из коробки”. Natasha пропускала внутренние сервисные коды. Решили дообучить её на 1 000 вручную размеченных примеров и 
            добавить паттерны через regex.<br/>            
            - LDA‑шумы. Алгоритм изредка классифицировал “редкие” сообщения как аномалии, хотя они были валидными. 
            Ввели правило порога частоты темы (минимум 50 упоминаний в час) и дообучили классификатор.<br/>            
            - Пиковые нагрузки. Пайплайн отставал при выбросах трафика. Развернули в Kubernetes‑Autoscaling и поставили резервирование юнитов обработки на 200 %.<br/>
            </p>
    <p class="project-details__desc-para">
            Результаты и эффект<br/>
            - +42 % найденных ошибок по сравнению с прежней системой алертов.<br/>            
            - 70 % ошибок удалось автоматически устранить (за счёт быстрого выявления и скриптов‑ремедиаций).<br/>            
            - –80 % ручной обработки логов: вместо 5 ч/день на команду — <1 ч. <br/>            
            - Граф клиентских путей позволил приоритизировать исправления: 20% узких мест закрыли 65% жалоб пользователей.<br/>
            
            Итог: Автоматизированный NLP‑пайплайн не только повысил качество мониторинга и сэкономил ресурсы инженерной команды, 
              но и улучшил пользовательский опыт за счёт ускоренного реагирования на сбои.
              </p>
    <p class="project-details__desc-para">
    
    
    <strong>4.	The proportion of non-relevant recommendations was reduced from 30% to 10%, according to expert assessment using 
    LLM inference.</strong>  <br/>
       Цель <br/>
        Снизить долю нерелевантных рекомендаций до 10% за счёт:   <br/>
        1. Кластеризации (HDBSCAN, так как он лучше работает с noise, чем K-Means: тест на 5 датасетах).   <br/>
        2. Генерации решений (GigaChat + prompt engineering, так как fine-tuning в 3× дороже при +2% качества).   <br/>
        - KPI: Косинусная близость (`E5` > 0.7, порог выбран по ROC-анализу).   <br/>

        Проблема <br/> 
        Обратная связь пользователей содержит:   <br/>
        - 40% орфографических ошибок (анализ через `aspell`).   <br/>
        - 15% спама** (выявлено правилами regex).   <br/>
        - 30% нерелевантных рекомендаций** в старой системе (из-за отсутствия кластеризации).   <br/>

      Результат <br/>
      После внедрения системы доля нерелевантных рекомендаций снизилась с 30% до 10% (p-value < 0.05, тест на 10 000 примерах).   <br/>
      - Метод оценки: Сравнение с ручной разметкой 3 экспертов (Cohen’s κ > 0.8).   <br/>
      - Почему LLM? Потому что:   <br/>
        - Классические методы (например, TF-IDF + K-means) давали 30% ошибок.   <br/>
        - GigaChat выбран как компромисс между качеством и стоимостью.   <br/>
       </p>
    <p class="project-details__desc-para"> 
    Архитектура <br/>
        Агент (основной поток):<br/>
        - Суммаризация: GigaChat + шаблоны для консистентности.  <br/>
        - Кластеризация:<br/>
        - Эмбеддинги – `E5` (выбран по тесту: лучше `BERT` на 12% по точности кластеризации).  <br/>
        - Алгоритм – HDBSCAN (min_cluster_size=5, подобран через grid search).  <br/>
        - Генерация решений: <br/>
        - 2 уровня детализации (общее/детальное) – чтобы избежать overfitting под разработчиков <br/>  
        
        Память (кратковременная): <br/>
        - Redis (TTL=24h) для хранения:  
        - Последних 1000 запросов.  
        - Кэша эмбеддингов (снижает затраты на `E5` на 17%).  
        
        Критик (QA):<br/>
        - Правила:<br/>
        - Если рекомендация ≠ эталону (есть в БД) → перегенерировать.  <br/>
        - Если cosine similarity < 0.7 → доработка (порог даёт precision 0.85).  <br/>
        - Логирование: Все кейсы с near-threshold значениями (0.6–0.7) для анализа.  <br/>
          </p>
    <p class="project-details__desc-para">
      Этапы <br/>
1. Предобработка:<br/>
   - Очистка от спама (regex + ручные правила). <br/> 
   - Исправление опечаток (`aspell` + контекстный анализ LLM).  <br/>
2. Кластеризация:<br/>
   - HDBSCAN, так как часть данных – шум (K-Means тут хуже).  <br/>
3. Генерация:<br/>
   - Для каждого кластера – 2 версии ответа (бизнес/разработка).  <br/>
4. Контроль качества:<br/>
   - Автоматический (косинусная близость).  <br/>
   - Выборочная проверка экспертами (100 случайных кейсов/день).  <br/>

      
    </p>
    <p class="project-details__desc-para">
      
      <strong> 5. Developed and deployed a CatBoost model for optimizing internal branch placement, achieving a 11% improvement in prediction accuracy 
        and 6% cost reduction in logistics by replacing manual processes with data-driven automation.</strong> <br/>
      Цель: Оптимизировать размещение внутренних структурных подразделений (ВСП) на основе предсказания потенциальной востребованности услуг в различных локациях.  <br/>
      Бизнес-проблема:  <br/>
      - Ранее решения принимались на основе неоптимальной модели и ручной проверки, что приводило к субъективным ошибкам и неэффективному распределению ресурсов.  <br/>
      - Необходимость учета множества факторов: клиентская активность, геоданные, экономические показатели.  <br/>
      </p>
    <p class="project-details__desc-para">
      ML-задача:  <br/>
      - Регрессия (предсказание метрики эффективности ВСП, например, посещаемости или выручки) <br/>
      - бинарная классификация (целесообразность открытия ВСП в заданной точке).  <br/>
      
      Данные и предобработка. Источники данных: <br/>
      - CRM-система (история взаимодействий с клиентами).  <br/>
      - Геоаналитика (координаты клиентов, плотность населения, транспортная доступность).  <br/>
      - Внешние датасеты (социально-экономические показатели районов, инфраструктура).  <br/>
      
      Сегментация клиентов:  <br/>
      - Новые (первый контакт ≤30 дней).  <br/>
      - Вернувшиеся (возобновившие активность после перерыва >90 дней).  <br/>
      - Активные (регулярные транзакции/обращения).  <br/>
      
    Feature Engineering:  <br/>
      - Геопризнаки:  <br/>
        - Расстояние до ближайших конкурентов, общественного транспорта, парковок.  <br/>
        - Плотность клиентов в радиусе 1/3/5 км (KDE-оценки).  <br/>
      - Временные признаки:  <br/>
        - Сезонность (праздники, рабочие/выходные дни).  <br/>
        - Тренды активности по месяцам.  <br/>
      - Агрегированные метрики по клиентам:  <br/>
        - Средний чек, частота визитов, LTV (Lifetime Value).  <br/>
      - Категориальные признаки:  <br/>
        - Тип района (жилой, бизнес, торговый центр).  <br/>
        - Город, область и тд. <br/><br/>

      Выбор и обучение модели. Алгоритм: CatBoost (преимущества для проекта):  <br/>
      - Нативная обработка категориальных признаков.  <br/>
      - Устойчивость к переобучению (регуляризация, early stopping).  <br/>
      - Интерпретируемость (SHAP-значения).  <br/><br/>
      
      Валидация:  <br/>
      - Временной сплит (train/test/out of time с учетом временной зависимости).  <br/>
      - Кросс-валидация (TimeSeriesSplit, 5 фолдов).  <br/>
      
      Метрики: <br/>
      - Регрессия: RMSE, MAE, R². 
      - Классификация: ROC-AUC, Precision-Recall, F1.  <br/>
      
      Оптимизация гиперпараметров: <br/>
      - GridSearch для подбора:  `depth`, `learning_rate`, `l2_leaf_reg`, `iterations`.  <br/>
      - Учет дисбаланса классов (параметр `scale_pos_weight`).  <br/><br/>

      Промышленная реализация. <br/>
      Инференс:  <br/>
      - Ежемесячный пересчет предсказаний для новых данных.  <br/>
      Развертывание:  <br/>
      - Контейнеризация (Docker) + оркестрация (Kubernetes).  <br/>
      - Пайплайн обработки данных (Apache Airflow).  <br/>
      </p>
    <p class="project-details__desc-para">
      Результаты. Эффективность модели:  <br/>
      - Улучшение точности предсказаний на 11%.  <br/>
      - Снижение затрат на логистику ВСП на 6%  за счет автоматизации решений.  <br/>
    
      Бизнес-эффект:  <br/>
    - Автоматизация 80% решений по размещению.  
    - Интерактивная карта с рекомендациями.  
      </p>
    <p class="project-details__desc-para">
      <strong> 6. Mentored team members and implemented professional development programs to foster growth and innovation.</strong> <br/>
      Цель: Повысить уровень технической компетенции команды Data Science в ключевых направлениях:  <br/>
      1. Мультимодальное машинное обучение и эмбеддинги  <br/>
      2. Process Mining для оптимизации бизнес-процессов  <br/>
      3. Практическое применение методов в бизнес-задачах  <br/>
       </p>
    <p class="project-details__desc-para">
      Детализация семинаров. <br/>
      Мультимодальные модели и эмбеддинги. Теория: <br/>
        - Контрастивное обучение (COLES, NSP, SOP)  <br/>
        - Методы сжатия эмбеддингов (PCA, UMAP)  <br/>
        - Квантование LLM (INT8/Bitsandbytes)  <br/>
      Практика: <br/>
      - Объединение LLM-эмбеддингов (GigaChat, Mistral) с транзакционными данными  <br/>
      - Ускорение инференса в 4x через INT8-квантование  <br/> 
       <p>Пример квантования<code>        
        model = AutoModelForCausalLM.from_pretrained( <br/> 
            "meta-llama/Llama-2-7b",  <br/> 
            load_in_8bit=True, <br/> 
            device_map="auto" <br/> 
        )
  </code></p>
       </p>
    <p class="project-details__desc-para">
      Process Mining и оптимизация взаимодействий. Кейс для разбора:  <br/> 
      "Анализ ошибок в цепочках сотрудник-клиент через Process Mining и ML"   <br/> 
      Техническая реализация: <br/> 
      1. Data Collection:  <br/> 
       - Сбор технических логов (временные метки, действия, ошибки)  <br/> 
       - Обогащение данными из CRM и систем мониторинга  <br/> 
      
      Process Mining этапы: <br/> 
   <div class="mermaid">
   A[Логи] --> B(Process Discovery)
   B --> C[Визуализация customer journey]
   C --> D(Performance Analysis)
   D --> E[Выявление узких мест]
   E --> F(Conformance Checking)
     </div>
              
   </p>
    <p class="project-details__desc-para">
    ML-модели: <br/>  
   - Бинарная классификация ошибок (CatBoost + SHAP)   <br/> 
   - Anomaly detection: Isolation Forest для "сломанных" сценариев   <br/> 
   - Оптимизация путей: Рекомендательная система на графах (NetworkX)   <br/> 
    
    Результаты проекта: <br/> 
   - Сокращение времени обработки ошибки: NDA минут  <br/> 
   - Упрощение customer journey: -3.2 шагов   <br/> 
   - Годовая экономия: >$NDA   <br/> 
    
    </p>
    <p class="project-details__desc-para">
    Результаты семинаров <br/> 
    1. Экспертиза команды: 
   - Освоение Process Mining: 100% участников  <br/> 
   - Внедрение методик в проекты:  <br/> 
     - Оптимизация воронки продаж (-3 шага)  <br/> 
     - Автоматизация обработки претензий (+25% скорость)  <br/> 
    2. Бизнес-эффект: <br/> 
       - Ускорение анализа процессов: 40% <br/> 
       - Снижение ошибок в customer journey: 15% <br/> 
       - Единый стек технологий для кросс-функциональных задач  <br/> 
</p>
    <p class="project-details__desc-para">
       <strong> 7.Лидирование разработки LLM-системы для автоматизации целеполагания и верификации целей .</strong> <br/>
      Цель проекта - Создать интеллектуальную систему на основе LLM (Large Language Model), которая:  <br/>
        1. Агрегирует данные о целях (индивидуальных, командных, корпоративных).  <br/>
        2. Верифицирует корректность их постановки (по методологии Сбера и внутренним стандартам).  <br/>
        3. Формирует рекомендации по улучшению формулировок и достижимости.  <br/>
    </p>
    <p class="project-details__desc-para">
      Бизнес-проблема <br/>
      - Разрозненность целей: Отсутствие единого формата (разные команды используют свои аббревиатуры и шаблоны).  <br/>
      - Субъективность проверки: Ручной аудит целей менеджерами занимает до 60% времени.  <br/>
      - Низкое качество: 30% целей не соответствуют критериям или стратегии компании.  <br/>
      
      Этапы реализации: <br/>
      1. Сбор и структуризация данных. Источники данных: Корпоративные системы, методологии Сбера, Внутренние глоссарии, 
      Стратегические документы, Презентации топ-менеджмента (KPI компании на год), Отчеты по выполнению прошлых целей. <br/> 
      2. Предобработка: Нормализация текста, Разметка данных: Примеры "хороших" и "плохих" целей (на основе ретроспективы их выполнения), 
      Тегирование по критериям.  <br/>
      3. Разработка модели. Архитектура решения: GigaChat (как наиболее адаптированный под корпоративный стиль), 
      Альтернативы: Llama 3 + дообучение на внутренних данных. Дополнительные модули: Классификатор корректности: 
      Определяет нарушения SMART (например: "Увеличить продажи" → недостаточно конкретно); Рекомендательная система: Предлагает edits 
      ("Добавить метрику: 'Увеличить продажи на 15% к Q4'"); Контекстный поиск: Сопоставляет цель с аналогичными историческими KPI.  

</p>
    <p class="project-details__desc-para">
      Моя роль: заключалась в комплексном управлении всеми этапами — от стратегического планирования до внедрения и масштабирования. 
      Ключевые аспекты моего вклада: <br/>
      1. Стратегическое лидерство: <br/>
      - Определение видения: Я сформулировала цель проекта, увязав ее с бизнес-проблемами (разрозненность целей, субъективность проверки).<br/>      
      - Приоритезация: Выбрала фокус на автоматизацию верификации и интеграцию с корпоративными стандартами (SMART, PEAK), а не на создание "еще одного чат-бота".<br/>      
      - Согласование с заказчиком: Донесли ценность проекта до топ-менеджмента, показав ROI (экономия 60% времени менеджеров).<br/>
      2. Управление командой и процессами <br/>
      - Формирование команды: Подбор специалистов (Data Scientists, NLP-инженеры). 
      - Распределение ролей. <br/>
      
      Методологии работы: <br/>
      - Внедрение Agile (спринты для тестирования гипотез)<br/>
      - Регулярные ретроспективы для устранения узких мест. <br/>
      
      Управление рисками:<br/>
      - Минимизация зависимости от одной модели (бэкап на Llama 3, если GigaChat API даст сбой). <br/>
      - Контроль за соблюдением безопасности (маскирование данных в обучении). <br/>

      3. Техническое руководство. 
      Архитектурные решения:
      - Утвердила модульную систему (классификатор + рекомендательный + граф целей), а не монолитную LLM. <br/>
      - Выбор Chroma DB для хранения эталонных целей (как баланс между скоростью и простотой). <br/>
      Качество модели: <br/>
      - Инициировала разметку примеров силами экспертов по целеполаганию. <br/>
    4. Коммуникация и внедрение. Мост между командой и бизнесом: <br/>
      - Перевод технических терминов в бизнес-ценность <br/>
      - Демонстрация прототипов руководителям для получения обратной связи. <br/>

 </p>
    <p class="project-details__desc-para">      
Результаты: <br/>
- Сокращение времени на постановку/проверку целей на 60%.  <br/>  
- Увеличение доли соответствующих SMART-критериям целей с 50% до 85%.  <br/>  
- Выявление 200+ противоречий в целях команд (например, дублирование). <br/>

      </p>
    <p class="project-details__desc-para">  
<strong> 8. Промышленное внедрение ML-модели оценки потенциала локаций для экспансии в новые регионы</strong> <br/>
      Цель проекта - Создание production-системы для оценки привлекательности новых локаций для открытия 
      точек присутствия компании с учетом региональных особенностей.   <br/>
      Бизнес-проблемы: <br/>
            - Высокие риски неокупаемости из-за недостаточной аналитической базы  <br/>
            - Субъективность решений при выборе локаций  <br/>
            - Долгий цикл ручного анализа новых регионов    <br/>
      </p>
    <p class="project-details__desc-para">  
    Технический вклад в проект<br/>
    1. Рефакторинг и доработка кодовой базы  <br/>
      - Оптимизировала пайплайн обработки данных, сократив время выполнения на 35% за счет: <br/>
              - Векторизации операций с <code>pandas</code> вместо итераций<br/>
              - Параллелизации вычислений с <code>joblib</code><br/>
              - Кэширования промежуточных результатов <br/>
    2. Переработала архитектуру feature engineering:<br/>
     - Реализовалв унифицированный интерфейс для работы с геоданными через <code>geopandas</code> и <code>h3</code> <br/>
       - Добавила автоматическую валидацию входных данных<br/>
      - Оптимизировал расчет пространственных признаков (расстояний до POI)<br/>
      3. Оптимизация ML-модели. Улучшила качество модели:<br/>
      - Добавила кросс-валидацию с временными срезами<br/>
      - Добавила мультимодальные эмбеддинги в качестве доп. фичей <br/>
      - Оптимизировала гиперпараметры CatBoost с помощью GridSearchCV<br/>
      - Реализовала кастомные метрики для бизнес-требований<br/>
      
      </p>
    <p class="project-details__desc-para">  
      Разработка production-решения <br/>
      
      - Настроила CI/CD:  <br/>
        - Автотесты (pytest) с покрытием >80%  <br/>
        - Docker-образы (многоэтапная сборка)  <br/>
        - Развертывание в Kubernetes с health-checks  <br/>
      
      - Реализовала мониторинг на MLFlow:  <br/>
        - Трекинг экспериментов (параметры, метрики, артефакты)  <br/>
        - Версионирование моделей с аннотациями  <br/>
        - Детекция дрейфа данных (распределения, важность признаков)  <br/>
        - Логирование прогнозов и интеграция с Airflow  <br/>
        - Кастомные колбэки, сравнение версий моделей, визуализация метрик  <br/>
      
      - Обеспечила интерпретируемость:  
        - SHAP-анализ важности признаков  
        - Автогенерация отчетов о влиянии факторов
    
    </p>
    <p class="project-details__desc-para">  
    Результаты <br/>
          - Увеличение точности прогноза на 15% по сравнению с исходной реализацией <br/>
          - Обеспечение отказоустойчивости (uptime > 99.9%) <br/>
          - Автоматизация процесса переобучения модели<br/>
          
              </p>
            </div>
            <div class="project-details__tools-used">
              <h3 class="project-details__content-title">Tools Used</h3>
              <div class="skills">
                <div class="skills__skill">Python</div>
                <div class="skills__skill">HuggingFace</div>
                <div class="skills__skill">LLM</div>
              </div>
            </div> 
            <div class="project-details__links">
              <h3 class="project-details__content-title">Private Repo</h3>              
              <a
                href="https://github.com/TanasevichPS/geo_embeddings"
                class="btn btn--med btn--theme-inv project-details__links-btn"
                target="_blank"
                >Code</a
              >
            </div>
          </div>
        </div>
      </div>
    </section>
    <footer class="main-footer">
      <div class="main-container">
        <div class="main-footer__upper">
          <div class="main-footer__row main-footer__row-1">
            <h2 class="heading heading-sm main-footer__heading-sm">
              <span>Social</span>
            </h2>
            <div class="main-footer__social-cont">
              <a target="_blank" rel="noreferrer" href="https://linkedin.com/in/tanasevich-ps">
                <img
                  class="main-footer__icon"
                  src="./assets/png/linkedin-ico.png"
                  alt="icon"
                />
              </a>
              <a target="_blank" rel="noreferrer" href="https://github.com/TanasevichPS">
                <img
                  class="main-footer__icon"
                  src="./assets/png/github-ico.png"
                  alt="icon"
                />
              </a>              
            </div>
          </div>
          <div class="main-footer__row main-footer__row-2">
            <h4 class="heading heading-sm text-lt">Polina Tanasevich</h4>
            <p class="main-footer__short-desc">
              PhD student in Mathematics and results-driven leader in data science and AI, specializing in scalable solutions and data-driven strategies
            </p>
          </div>
        </div>

        <div class="main-footer__lower">
          &copy; Copyright 2021. Made by
          <a rel="noreferrer" target="_blank" href="https://rammaheshwari.com"
            >Ram Maheshwari</a
          >
        </div>
      </div>
    </footer>
    <script src="./index.js"></script>
  </body>
</html>
